{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "dcaiti",
   "display_name": "Python [conda env:dcaiti]",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = '../res/model/config/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('../res/testimg/input')\n",
    "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
    "TEST_IMAGE_PATHS\n",
    "image_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
    "# detection_model =  tf.saved_model.load('./model/TensorFlow_model_zoo/' + model_name + '/saved_model')\n",
    "detection_model =  tf.saved_model.load('../res/model/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'detection_boxes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "detection_model.signatures['serving_default'].output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'detection_boxes': TensorShape([None, 100, 4]),\n",
       " 'num_detections': TensorShape([None]),\n",
       " 'detection_scores': TensorShape([None, 100]),\n",
       " 'detection_classes': TensorShape([None, 100])}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "detection_model.signatures['serving_default'].output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image, threshold):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed >0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference(model, image_path):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(model, image_np, threshold)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      min_score_thresh=threshold,\n",
    "      line_thickness=4)\n",
    "\n",
    "  # display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unnormalized_boxes(image_height, image_length, boxes):\n",
    "    unnormalized_boxes = boxes.copy()\n",
    "    unnormalized_boxes[ : , 0: : 2] = image_height * unnormalized_boxes[ : , 0: : 2]\n",
    "    unnormalized_boxes[ : , 1: : 2] = image_length * unnormalized_boxes[ : , 1: : 2]\n",
    "    return unnormalized_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_image_from_box(image_np, output_dict, threshold):\n",
    "    #image_np\n",
    "    im_height, im_length = image_np.shape[0], image_np.shape[1]\n",
    "    boxes = get_unnormalized_boxes(im_height, im_length, output_dict['detection_boxes'])\n",
    "\n",
    "    extend = 10\n",
    "    #1: person; 3: car; 4: motorcycle; 6: bus; 8: truck\n",
    "    detection_classes = (1, 3, 4)\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if (output_dict['detection_classes'][i] in detection_classes \n",
    "             and output_dict['detection_scores'][i] > threshold):\n",
    "             cropped_image = tf.image.crop_to_bounding_box(\n",
    "                                                    image_np,\n",
    "                                                    offset_height = int(max(boxes[i, 0] - extend, 0)),\n",
    "                                                    offset_width = int(max(boxes[i, 1] - extend, 0)),\n",
    "                                                    target_height = int(min(boxes[i, 2] - boxes[i, 0] +2 * extend, im_height - boxes[i, 0])),\n",
    "                                                    target_width = int(min(boxes[i, 3] - boxes[i, 1] +2 * extend, im_length - boxes[i, 1]))\n",
    "                                                    )\n",
    "             cropped_image = np.array(cropped_image)\n",
    "             #save_image(cropped_image, i)\n",
    "\n",
    "             display(Image.fromarray(cropped_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_image_from_merged_boxes(image_np, boxes):\n",
    "    im_height, im_length = image_np.shape[0], image_np.shape[1]\n",
    "    extend = 10\n",
    "    for i in range(boxes.shape[0]):\n",
    "        cropped_image = tf.image.crop_to_bounding_box(\n",
    "                                                    image_np,\n",
    "                                                    offset_height = int(max(boxes[i, 0] - extend, 0)),\n",
    "                                                    offset_width = int(max(boxes[i, 1] - extend, 0)),\n",
    "                                                    target_height = int(min(boxes[i, 2] - boxes[i, 0] +2 * extend, im_height - boxes[i, 0])),\n",
    "                                                    target_width = int(min(boxes[i, 3] - boxes[i, 1] +2 * extend, im_length - boxes[i, 1]))\n",
    "                                                    )\n",
    "        cropped_image = np.array(cropped_image)\n",
    "        save_image(cropped_image, i)\n",
    "\n",
    "        # display(Image.fromarray(cropped_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_small_box(loc, w, h, thre=0.1):\n",
    "    return ((loc[3]-loc[1])<thre*h)&((loc[2]-loc[0])<thre*w)\n",
    "\n",
    "\n",
    "def merge_bound_box_near(output_dict, w, h, thre=0.1):\n",
    "    '''\n",
    "    merge bound boxes\n",
    "    Args:\n",
    "        locs-->list: [[x_min1,y_min1,x_max2,y_max2],...,[x_min1,y_min1,x_max2,y_max2]]\n",
    "        w: the width of the image\n",
    "        h: the height of the image\n",
    "        thre: threshold for judging if two bound box is near enough\n",
    "    '''\n",
    "    locs = []\n",
    "    boxes = get_unnormalized_boxes(w, h, output_dict['detection_boxes'])\n",
    "\n",
    "    #1: person; 3: car; 4: motorcycle; 6: bus; 8: truck\n",
    "    detection_classes = (1, 3, 4, 6, 8)\n",
    "    for i in range(boxes.shape[0]):\n",
    "        if (output_dict['detection_classes'][i] in detection_classes \n",
    "             and output_dict['detection_scores'][i] > threshold): \n",
    "             locs.append(boxes[i])\n",
    "\n",
    "    # print(locs)\n",
    "\n",
    "    flag = False\n",
    "    while not flag:\n",
    "        if_modify = False\n",
    "        locs_num = len(locs)\n",
    "        for i in range(len(locs)-1):\n",
    "            x_min, y_min, x_max, y_max = locs[i]\n",
    "            for j in range(i+1,len(locs)):\n",
    "                # if i == j:\n",
    "                #     continue\n",
    "                x_min2, y_min2, x_max2, y_max2 = locs[j]\n",
    "                # if (not is_small_box(locs[i],w,h)) & (not is_small_box(locs[j],w,h)):\n",
    "                #     continue\n",
    "\n",
    "                #same\n",
    "                almost_same_thre = 0.01\n",
    "                if (abs(x_min-x_min2)<almost_same_thre*w) and (abs(x_max-x_max2)<almost_same_thre*w) and(abs(y_max-y_max2)<almost_same_thre*h) and (abs(y_min-y_min2)<almost_same_thre*h):\n",
    "                    locs[i] = [min(x_min, x_min2),min(y_min, y_min2),max(x_max, x_max2),max(y_max, y_max2)]\n",
    "                    del locs[j]\n",
    "                    if_modify = True\n",
    "                    break\n",
    "\n",
    "                #near\n",
    "                if (is_small_box(locs[i],w,h)) or ( is_small_box(locs[j],w,h)):\n",
    "                    y_near = abs((y_min+y_max)/2-(y_min2+y_max2)/2) < thre*h + (y_max-y_min)/2+(y_max2-y_min2)/2\n",
    "                    x_near = abs((x_min+x_max)/2-(x_min2+x_max2)/2) < thre*w + (x_max-x_min)/2+(x_max2-x_min2)/2\n",
    "                    if y_near&x_near:\n",
    "                        locs[i] = [min(x_min, x_min2),min(y_min, y_min2),max(x_max, x_max2),max(y_max, y_max2)]\n",
    "                        del locs[j]\n",
    "                        if_modify = True\n",
    "                        break\n",
    "\n",
    "                #include\n",
    "                extend = 0.1\n",
    "                if(x_min<x_min2+extend*w and y_min<y_min2+extend*h and x_max>x_max2-extend*w and y_max>y_max2-extend*h) or (x_min>x_min2-extend*w and y_min>y_min2-extend*h and x_max<x_max2+extend*w and y_max<y_max2+extend*h):\n",
    "                    locs[i] = [min(x_min, x_min2),min(y_min, y_min2),max(x_max, x_max2),max(y_max, y_max2)]\n",
    "                    del locs[j]\n",
    "                    if_modify = True\n",
    "                    break\n",
    "\n",
    "\n",
    "            if if_modify:\n",
    "                break\n",
    "        if not if_modify:\n",
    "            flag = True\n",
    "    # locs = np.array(locs)\n",
    "    # print(locs)\n",
    "    return np.asarray(locs)\n",
    "\n",
    "def merge_boxes_and_clip(model, image_path):\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = np.array(Image.open(image_path))\n",
    "    im_height, im_length = image_np.shape[0], image_np.shape[1]\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np, threshold)\n",
    "    merged_boxes = merge_bound_box_near(output_dict,  im_height,  im_length)\n",
    "    clip_image_from_merged_boxes( image_np, merged_boxes)\n",
    "\n",
    "def merge_boxes(model, image_path):\n",
    "    image_np = np.array(Image.open(image_path))\n",
    "    im_height, im_length = image_np.shape[0], image_np.shape[1]\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np, threshold)\n",
    "    merged_boxes = merge_bound_box_near(output_dict,  im_height,  im_length)\n",
    "    return merged_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(cropped_image, image_part):\n",
    "    path = '../res/testimg/output'\n",
    "    Image.fromarray(cropped_image).save(path + \"img\" + str(image_id)+'_'+str(image_part)+ \".jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 388.06723 1627.9656   437.98053 1677.7578 ]]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=69x69 at 0x7F120F04AC18>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEUAAABFCAIAAACT77x+AAAgtklEQVR4nE17WZMkW3KWb2eJyK2W3u4yoxlghBCSYYaZ+Hn8AR75WbxhmBDGINDM3Ht7rarcIuIs7s5DZPdMWFm+dFdkeLgf92/xwv/6X/5z77337moAgIiIyIjubma999ZaK7W1pqroTdWXZTmfz+fzeZom7e7uX758qbWXUlprX38LmkJBiBHGMcUYiQjMHczdA4u7ImJgFpEYZbPZbDabWCillMch5xxjpEAkTERPz8+SZLvdHu7vHh8fH14/PD4+brfb5XK5Pbw7MhGRlFJUVVXBHBGJaA3JzFTVurraGpuqonspZZqm62Wersv1OpdSerdlqapq5gAIgIiEaARw2MQYY845RAYAd1dVd2NmgICuZlZKMTORKNL67GruCL13J3R3REcmd58me3l6ev/zz7vD4d27d9//8O7h4WEcB3df7+xdnV1aawBAREBAgADAfxGMmQEAAzKgOahCWdr1Ms/z3Lu5Y2s6zwsiEnGMDE6I+PW5fRgGZmZmRHRQM1MmQFdVRHTg3te0Q10KAYqyE2JlBV/vg4hATkS999aKu/fawExbOz2ffv3bX43juN1umXlNlBDR+n2IiA7uTrB+ogEwICF1ojVvqt6altJ6NwAgYkRyd1NARGZmYSL5luQMiIiOYGaqSA6EiEBzLUBkqlbVvAOStd6gOEXsxF1ERCKLCDMT0fV6ZiQK2axb1+OX5+UyvQ8hRn716tUwDMMwiEgpRUII63cDAJgDAAKEEBTQ3QHM3YkIARBgnudaq6qZee+qqm5IKCEHIvoWCRExB0QcWl/vbOSKqt4BwAk5jQZWrHRzBwjEggRqQKCqpRQASJ5ElRABoNZ6u7OTaiut1WUBgH/8x398fHx8fn5+eP1qHMcYozDzejzcfc0PAKSUwBzU1MHMEGCt0XleWutrNdZaa+m9dwB4fHxEvJ0Qd1wLGBGzFyICQgBQ1e7d3QGRx01r9QzYa2PAyBKIDYGDuNraV+Z5BnBT7b1vNgMRCRIRsWAIIccUQvj973///v379+/fv3739u3bt+/evRPQBQhFCIncvXdtvQcip4bBhiTW+mUpfT5ba1sCEXSGDr1Yj+K73T7FQdXMOjoREZpbd3KILEdugSUKowOwBcAgLMI5xloh0iwArXfAizVr2suStTuiMKVuqB2D5GHYtJqHYUhDQnH3ZmQqIkn+7vFXzXU5LZ/qT1obpSCICIjrqzWzNVGtNVMF827ea+29E1EI4Vs5pZSAxBGIiJBa66rq3sUDObjD2vpTiEi33OJfDIPNZths0nY73pfDXOfW2rRMy7I0C8VLWcrSSm2mijEM2to4fq+tzt6ADNBCIEIn9BgzWJ9Ln65X+/xZtoMwiYKDgwO43360W+AgkUGtLbV3bbV/K0tmzjnHTAbemzXt3bp2dXcgZEACRBREDETrsSR0JgoiSViYckwSCMBaS6VtWqvXZVyWZZr7hIx9mlpldyYaQkhB+nLtAMgoAWIUJNKic52fdMLAnQwAmmlrTRBxrXr7+rhr9ccYYwhtKao6z/P1cum9l9LcXUQYooLX3txvPX3t0QTMxIhIBMAA3QFACIRCEErCQTgwBgQBAgRDTMwxjNvNYGZLsWmaji/X0/FaiqpTDDnGPE0TMcQoKeYUiBB6Ka3Xp+Kbu93u8X54OOxfPbz+/p18PcTu4N/wARGp6qWU88vx6dPn49NzmRdVdeMQgsTg7r233ns3XaeE41pXiijIBAAAQGDMHJiCSCAMIpGIAdCBCYgIXIyIA4UQmJlQ5nl5GU9P6XS+zGVRVe91TuQIyNatLX1qSGbWrff9/ev94/3jd2/v3r4+vH119/gg36rI8TZDiAgArtfr05cvn95/OL8crfV11Lo5EaHSmpze7Fty1rNnRAoKauDq7oLGyMLM4GQAvYOwBPZWQ5ZxGNyjqjra+r3oJCnj1sRxm4ZadZnrXFsIQVVLW+p8bZMyY85xm+Q3v/lN3m2Gw04it1ZOpxdRdTNwB2Qi4nVcEGBrl9Px8vJyqlMJzGvGtLduSrUZgpp10zUKVe296YoAHXTtB7CMwRFMwDuCmUfhkMYxJwLb5nzYbYmoW0P0NZ66NAA6bMayv1N1c661z/Os3RS81nK+HKflKkL39/eHw+7HX/0qbUceQmVo6L13udXa1+vWwWCdiRxCoAyBWVuvtaqRlqrgRETMjqCqVXvV3ntfAd56HKG7u5MaupEZo5NB4pyC7MdBmA677WG/ZWazHkLIOUkMbSpEZAatqRsC8FzaNE1mICE46PV6vVxOJPjw8PDwcA/jfnt3yPttI5i1QWCJMdJaLODr1ENECTGltN1ul2lacGJEimkcx+NxXpal9dJVoTUDB6YQwjRNkmJGWicyulMARAzYa12st21OOQ85BkY37Xf3D5shbfJw2G/TkEIIRICItlWA9SCSIQEQOALAOtZba6WX3rtqX89I3u4QqZXaBIGNHAUREfBbg1qzxEi9dzNbWzMjauulFHc3BHBERAM3BzBT1RjjWqho7l3dXcEECMBEKMUQQiAGCTQO+XDYDUPajHm3HbfbTUqBmYmBiMIQb63JXQ3c0d0VkJlvNKRnXeEWqLtfHLQ1A0OWIWYQFgD6OnoAHG7zFLR3MwNCoUBo3rQvc11BHSIiIYCrqqk7KIsgrL/bzbubIWIHu2GTHHOMQhiCDEPa7kYRSSlth2EzJJG1eFFE0CIAuMOtuziuAKq7ITozAhAxuJM7u7sU7K2a95TDmAeMImuD+sss3fCB2frKe+/edcVUgAGRkQERARTNHGGFAw5g1m+Vi0CIRBQypRBSCinHyByjIKKqKlYzddCVWKg2dyEG7wTfHgMB0cEQEco0u6OZqdstP4buniSbcUNgokiMLLd5CgDf2KiqBpYVVgNAa62XamYi0tpt4AKiAwP0tT7Xl3LDziKELsTMvNlGIooxDUPOIWZmA53na9huVJtqa60xApIDGnUgTGswgDdkrHh73Y4OgGgrAWEFdfBoEIkBgdRrmRH6OhtuV++91tp7xwjgN0Z1I7RmANCa1qbmHYkMQVXNbQUY9PVidEYSESGWwEwUgkiKIQgjAljv7S/uXFGInMDMTIluo2xN9EoKzZ2YGBjYQckRWb9xUiNCIey99otBC8IUEIFIHMkd3ZGoM4tqU/X1p9Ze5qWUUhRra6pKgoag3m/HCYEFgZmRiJDwxudqKyISIai2joAAAQMRAVjXuiwTM3oUEQZwA1fWlcauwehXfExBAAgA3NAAHR2Q3O0bD1Xz3jogrIeRAICERUREVLXXZq2r6pqcUsrxeDyfz3l7V2pVVezghAYOYMCECAEQAJDAYMXrAOYXvQ4pRwmNyVtXxCzMQdZCWJaFBQGzgagbG8eoALACdFzZL5EhtNYACBEdaA1sjUOIjdDJmLwjAKK4X/vSgRiFU+QUB1M4n6/WOktQpy9Px/e/fKylRBYl4M0AXWtr4B6ItfV6We72B22tmRobRKFE7tQBgGBAaa0XQ2JR92ozxZGUEJpL0yCdGMC7G3oDP6/vVJBVtauCOwNGiczshr13AF/fSOltdC5d3dySAEo3ktaau5u5myIQIiMwEYkIIq5vcW0SVY3roqvsZBYoMBISdPDT8dmsozkz6pDQx2FIIQRGYSQ0b9aoa1el3i85jQ8P67jrvVNrQMCEQoQO6ECACIjEJGuisbVmDqq+0mFfeYD7+XrhIaFI7/3SSiUSdwcgRDADByMiROi9rwldI0kppRjJwYVdtdTSazMkE0EH9p5TImcAgPVYtaVjhy7jITGBtm6qiMyqYrYsC+Kto65SGRvTOmi+KhnfYD4AIFKtdW05IjdC0FqrtUIzidKrPS/XU2+8GaU1de9A6EiIaNbdVLUJCRG5KwDEJIyC5udyEaQhMAcRJEZidPTYW4ksITAROazSDzC6MIJarxVUUwiBacxxM95ELPl6raARgP4MUOgWDAC4W86ZmYlknYerLLMsi1ZTLS9l+jJfLadXh52klHrvSExBELn33qouy5Ikrb3FzGqtBM27xghMGBjB3Moylxl6J4dxSGRCwCLCghLDKnAurZhbL4XNJOXNZry7Ozw+PuacY5JhyDHnkISEVsDjaubgSEhA31QAtZxza62UZQ3jer1O01RrvU71tExfpnMVevz1j8N2I/M8qxogQiV3XON5enoa4lDKwswi5O6tdzS/34zz5TqdJ3QYY3h3t7s/3B0243bMrbVlnqZ57loRTbyLIaoiAIPnGHfb8f6wf7i7v7u7i0whckoxSvg6tpiIkqS1qETCiiRqra32T58+XS6X4/G8hmFm63+rQC/X82Jt9+r1m++/29/fyel0RkQ1K13X3oDAMcYQgoi8fv26XKfW2jLNRDifz9fLGQ1+9d2bv/t3f/O3f/273/z44+Nh/+nD++v1+vT8+enp6Xg8XqdpabX3vgmBiFxkO+S3D6/uD7vDdrvJaYWnRDeVHJ0IkJkjibtb6/PSaq2Xy+V4PF6v008//XQ+n6frAgAxxmEYDodDDhHHcXRNkd/8+OOrN69DjPLDDz/EGEvtp+tlWRamkFIiosjRzHJKy+X65cuX6/li5jadrcLj/fC7f/Xb//jv//bH796x28vH9+8e79smP4zpdHe4XC7H8+l4Pk3T9OWyBBEG3G3Hx/vDdhxzziISk6CDgfbewQEIQxBGmud5zck0LafT6dOnTx8/fjweT1++fKm1Esp+vx+GYbPZHA6Hw+EwBekEkMN2v2Pm6zILArshEeWcgyQRCSGcz2cU3O124P50/+n161fg9vz5y3YceeP/7m/++j/8+79l8N//0/9k67shf/7Tv8TAiNi0C8DddmO91Wk6jENKaTuMm2HMIQSRHATcwVzNHC3GKEnc/XQ6L8vSLvry8vLp06eX59N66EsprXUiSiESCbj31phoyHkzjsZkhDjEGGPvvZjJ184ohAZ84/EiguRIPgzDw8PD8Xi8Xi5P/nmey5ACGpHDkFI+7BNBFn5++uhN1az2Zu4GmJge7w/nS48pDCltUhryMA55yDmEVaF3R3b3tpTrMh+Px8vl8uWnp9Zaa40AxpxzjC2lVvs4jogoEodhuLu7e3x8vN8fxpTDdjO2akniMKqItC7uaLbKb3++EHHF2kR4uNu9ev3w6eN7sz7PGlnWjgcAd/vDwOi9xIdX8zxN00TupdauzkH228OYbtLXMKbxVmz0TZQ0cHevtZzP5y9fvhyPx+VaYox3d3fjuA0hrN1Zu53P5xDCMGx2u91+v9/tdiklEcnDGHPSyB5CASREWc0gA/+GAleVZ6VUiBRj3O/39/f3h8Phy+kjANZqp9PlftwcUiSiWtuYcl1mMGPkMeVqLiHlzZg5rfrjeohDkhuy9o6I5LCSfAYUJGZ+vH8Yx3G32w3DhojsK60z1c1mc9jf7/f79QQSETrU6wyBkVBLU0ZEFIAVVsM3Y8jMYozozszWFRHHcXz9+vWPP/74/MtnA5nm5fnpfDeOD9txJGytvUzX4+m4LEseN4fdgXOWkCRFXMrKCyXFnGNIEREdYGnLylDULYQ0juPDw8MwDDbDigDmeV6hyYoP7u7u9vv9w/2rYRhu3pQZEZXrxENy9G5qwpTiV68GYMU+iL6GRHCTIGpdiGC327x6/ZDyYI6XaX4+nl/f7VpTGHMIqaulkBBos90/PLzK2x0Jd4Np+uAOCABq3hWCcyQSdgh9zT/cxk8IYbfbtYuuX7ostdaqqqsbtt1ux3FcK3BtgKsUFSgyMiA3dwJkYkH8M81edd+1aoWImXuv3womxjgMm7JM18t8vl4v01JrBd/GGDcpDkMqpQDL+oyObNbrUlbdtJSyLFMoKQ05pAirr+QOCCv2SSm5O2RfnZLAJYW4fjWzBBZt/Xq+rO1qZVfgkGIMKWng5ooiFKLcQI27+UqedA19/RQRis7MZV5EJKahzMtc27JepalqJAHvISRVn8qilwtKkhhr77UuAKBuzVTNOIY05DTknPOKPVe79xuEK3X5aofxmo0b5UWstZalrSpazpmI3B2RZGVJ6IFFJMiExVVvhJkZgVcNrSxtmWsMIaYtIu7u5F//zfZP/3L6+Pm5Nv98boen5d257ff06mGP6FEo7FruXa2re5mn3jvvx3kutSoyA/Nc6mWaCeXh4UFEYowhEAh5RxQmIkzMHBK5k1MDdAoiTCGyxBijRHfvtdbzFQGYeQgtBIiSZZAiPulZGNCZAYCJwAnMza1MBQCC3NxVAjAiZh7H8f7+frlczufz+/fv/7Tf3qX0eL/r2lNIQ94iebdeWpuWubVm9mdpAQC+ie7LsojIaiut029NiwxsZq66dnlGiTEGjtZ6712bruJL71Zr1d7lcOAU2cxab6atg6QYV6nWmgMouoPhZrPB1TXhgADLspyPp8vl8vnzZ0QkxPl8fXb7+eefHzf5t3/1w25IQmxmvdWmTVVdjZEcca2lr0wGzQyBV7FytdbXlrz+K+ebCRuIRUSChBCiRApxWZZlWhBxiMOQw6pGBWA28mpzq2erC6GAInYgu/EnAkbEu92+1tpat65LKZ8/f/7Tn37+9OnTH//lD4w31uXuT09Pv/89PN7vfvfb39iQxjEzYeAQWESkh3BqcwjBv+pha9HfTIrVJzZb3+Y6x8uyhBByzpzGr0BbJBACw3Jb1hCU1dBmZp6bIlxra1qaq2UWbz2whBwCMQCtd//y6UsppZRiXUtpayS//PJLa01iyjly74h6PB4vnz95L9enp++/e/NXv/rx8dV9jlFV23RZJSFmRmB3d0MEJkTDlQWv/jl8c2gQsbe2xpPHPKYxpRglCvPpeCml1N7KXC+X6enpSSQys52mpbeLtxLQtzk93Mkm5RSHtZrP5/Pzl+fL5fLLL7+4OwMTUe/95eXlerq2peWQwU04UErYCiIu3d6/fx/Iji9fTi/P94dDCOzuS6u11nGbU0prp/rarOib6g9ogE5849dElEMach6GIeccJKy8eu3g6JBSsu4vLy9/eno5vxyXZfFLreiwSds3j/sf3vhmlChJe5+u18vL+ePHjx8+fHh5eZmmZb0pAV+v15eXl2WaGQmZa6nMPGw2tmDmwYcUBVtrLy+nupQ/EgDY2ruY+c0Pb/wvhAFmXoPpvSP5t9F3K3WiJCnEGCIjYtVqzVpT64pOMabdsBnHLXMopb08n6ZpEZFhu3n41Xdvf/vr3XevZbeVcpnP5/PxeHx6enp6enp5Pk3TNI7jmDebzaaUsizL6XSZ5+IOMQZsZRzHJFjQhsAYGXRBEhZJKQkBIkbhlBIzfzXL0MwBbk5r7x3QVuX2K/y9Lf7kIcUYJQRm7M1Wdt2rDsNmbY/jsI0hMwXh+LTbTbXfvXn167/+12//zW/C3Xb2Lv/7n/7Xhw8fjsfzii8AYIgDOVnry3WZyrIsdT2Ia8F8990Pm91GwEi1LZcYeMi7/d0uR4k5BSZe7TAndF/V42maYowissqnALCWd2vNXUVEJK10eG2GXdv1er2ep2VZ0JA55Dwuy7LM1ftNrGWWu7v7+Xq9/+H74fHheZ4cer7fy3SZl6nUpSy1aHciCgFFIgC5e611mUurnZBTSiFmSVFCYDQZEpNn0iy0PewzURRhcHS9gRmjshQRQSRmZWYA/irZ+Nfyw69LOkAE3Xtv3buXUpo2EQkhJUnX6xWRGASA0JFDyIhJkwjJdoPjgEk0cWUSd48x56zuWL06ECLfxMSuZanLsvTeUxpSGqpZ672ZIgEHIc6JMQfc3O0TYkBk1V4b9LbSqXVu/qWfuabIHRBR5IZ1RGjtcitt6dYRMYQAhqsq0k3rXOZ1Q22ae1dTAIDx3/7OY7QYIYoHaohiBjdOH/Jluq5KIrM0tVrrPJem5kjq1rQvzUhYFrIVqcQQksQgQMKMwsxuRGSFTBs6uGFvBt6+SYcxRhFCWvU3FhEiYL6Vae8VgJgZmb3bstTz8Vim8sc//lSmdrlMy1TcYEhpHLYppV2KwAxEIAKE4Cjv37/PaVwFnRijO6h77b21dpnmuSxre1pqXWoPeVNbuxZyV2PEiEWByY+Xs+Wwz0MIIoDNgRqaWQi3Qd61fvM9YxTi+NVeASICvFGvZp0AiYQZkWD1VJZWETHGuN/LfotBYs7jdhhTGuL+LqWBKYBECNDA5f0vH9eVOInB7OZhLcsylzpNU2kNSJi9taW0eZvH0pvPuhSLBFlgEsyMZcx1HPnOCTaZkJndgd0zY2ultda7NWhrBO4akwDYt7PkdltK+brs1d2dUIYhBY6Hg/393/49OIOhGWjV1lSrquqVo1arS0s5cAgOLgBwuVxqrRIjIhoAERmgqjqiSARSIHQEZEIiA1tqRW+VvKAtZBnBavJWIyE7YooBbzuConBbaCBy0K+T9EYZb583fZiYkQKo3jxcQogchs2wolI3coXeTUHNrJm66k//8gfZDIfL6e77t7s3dzQkGcfxeDwez6fVuERm5rDd71aEawakfdWac85dIli3DujG5DfmAXad58Q0LXmMMSCQBGIRYnJd5wYROfwZHPyZPH7dkHVXAF6WZgZm5grgrWEVUUZ5+vS0zO1yup5P03RZdwodAD5TCmN8OL37K8GwTducxIPwkG3xdf5YAYDZhL7O9a8WGBMi3Bc0jhV4USnmleml91qX+9191VAv4dLbQ6bXm3A/xBSDBGy9EAQOkRkNoZS5lCrXEtWM42Y7xBib6lKmMpUmG3JAMAZkRAX06s3qNC/zUp+m6flyLtVIEg/RgCBvP7w8//z7n46Nz5Xe/uBiZnHIISdVXZZlnsq6wbty7JWx2G1jQ8MYp2XuZhIF0EutDr4ZhmWafHZLUfbbQ47OVK23uYbB182Hb8tvxEGiHc+npIMTApMBiVAeNimP1cPqndJNDwAGJOBx3DjQu++pdTdnQ6qtl9L+2BGDfPz86X//8//55z/9YXvYyWWeVm1gfXSmIDGszOSmwPjXXDl01LktVfsQt0xo3hEghbi0pbU6m9YxO7EyTeDaaqbb9pWwKJK7NtOm3sy8VAdcWk9pGsdxs9nkPIiImWlvoEbojBRZmEPOg3BETo4IGJZqL8fT8XisxwLEyEFLPZ2OX04vUmttrS3LEiSRcMwp5rRq+KvLCVC/oaxzPTesKGDQwEDYvWubpyEGRwgEzjRbv2gfgvCY5+V5xZpN3Xtdb1i1xyitttM0194AYBiG+/v7/X6/PWzJAdAJUJCCOJEAeFPtVr2ZGqovS9XT6fR8Oj691GupFONhGB4jV+0ybnallNZ710X0th+8mkLfzu43ILy0adzmFWiC+Z5znZfr5cwxIZjHMJfly8uxmb5+9XC/2wFeVbW0VuZSSqn9tr6gZzXw1WluvTPz5vPzMAz3D7sYOKWUY8ohDjmn1LNkxA4s4KhGClibnab5upSqDhIEEJkkCQPI3cP9six1WadEX9flc85/1neIVq/TzIRofxhTiHVp5JA5TUG4lOl6BVcPwVtdlvlaFs5xOBz2m22tdVG71nY6X2ptwMTML8+n1U4HIHOeq13bhU7Th+PTmPJms9kMeRg2mzyM45hSR+QYMnNoCq3pdVqOx/PxfP750vf7/Wa7U7emtbuJOxKJRANCILTq4Dd5GhFZvgoADEiYXNA9CG/3aeCYSPqQX+Xxy8dP5+la69JLXZnohy9PRvy77zYeQ+LdgDCbXl9O12nS7qUU7MKshDcB0BC8a7leY5zzZU4xCsec8yZvcs6IlNKAHFq1eS7XeZquyzzP7ysYIsfIMTAlYZLj+fxNjlhBu5lN06S99t598a/CUmBmqL1O15Foe/dwN+wiEKWRDo8Pw/jhw4dPL08dnFhc7fhyal3f3v242Ww2+23YbJTDubTr8+l0uW6321b70mYzICLhlU3w89K4Gk11Xc9hDjkNKSV3XHFNra3WvoJRd8fNbiqLPn8JIewOh8N2K+frJCIpBXAoTZE8SYgxVrCbdUGICHVZpmnaDmDsy2WanFM1zgMZm/ko4fFwhw6nZWlgRuQKl9P83/77//jxxx9//etf393dvf1+w3kDYYCfP5zP527uwA54XcqyXNbFtCv6PJ/NLEgCoGk5qplwcEdCEYlDzCmlGHMIgYi11dhTIpEYgbD2JiRiAL0bgKkpO3W2EIIIpRhrLQAAdvv7Cq0G5Etbzt2x9J6XzCFSAIAU4sPDw869uVWAqr2rfzr/4adfPtbub9/Wx8fHu1dvgPP28Ph//u//m+d5npbeu3NQbEvpU72cCHoHACBtAKSGCkJKAIRIoWNFi6ihV2ZFxOgCTE5ooBwk5yx3dw+tldvf+bS2qNLiSUIIHEJYdxG0V7UGaIFXjaK/lHo9XjKHMeUk4WaBhYAsAGS9eXd3C8P+PPfrnz6c596c3r55t7t/5DQ4py/PTx8/fjy+nL2qITXX7rAYiAAgN3UzQBJHVgMJGQAdyTgoMCCqu5tBNQBT7+pdYkhDlst0XeemqjbTuizaa00pCccgzMwEFAIiiogrOHj3rrVqqwEp12VIab/dRZIo5IS9r0c0RKYKcD6fj+fLtLSmNBV7eHiIMb79/oeQB3M0YowSN4MbIuIcMecBkK/zstSOIE2tLE3VAZCRJEgKUZhX0IfYW2s2ORGlaxqGLEspKcYQ0g01dm2trI5AbzWllFMIIYTAZnZ8mXh9TyE4QQesgZzd+jIwDs5o3s1aUwAkonMrS7FS/XK9zssfL1P5q9/0N2/ehJS2+7sfJe7vHq7zpKq3VpQh5ayA59N0uc7z0k+X+XS6fPr0xd0JOaBEwRAYTV29E6iqVV0WXqY8TZP8wz/8J4BVJ7ieXo6fQGtdRKh1L625K3hal1XJIYw7dwdQ6q1rq9oaGGr9eDlth3HX95Gj9l7m2moHhTmygQOGrv3p+TjNpZlPS3n79u1+v3/3/Xev3ryepqmUouCIvr0TQjHA6a6dztfPT8feysyWAoAhI+QAY6QkBO6926WatW4Ipc7XSeIlyLsfvi/TfDodtfeYU8655MyMLgLqa+/uvQpSCCE9PKo2tWYEaqDmqs16hyw14AKq3t1cwR3BCLoiACKwcFy0Pp/O9f/+4eXl5XS+vvvuzTt/l1KkIJHQXd19YOvWCGk/RnK7nM6gi/VlO0QwI8Ah8hAhsFvvCA3BAAzUeoXr9YoM/x8JJOQ5Orzc+AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image_id += 1\n",
    "  show_inference(detection_model, image_path)\n",
    " # merge_boxes_and_clip(detection_model, image_path)\n",
    "  print(merge_boxes(detection_model, image_path))\n",
    "\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(detection_model, image_np, threshold)\n",
    "\n",
    "  clip_image_from_box(image_np, output_dict, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'AutoTrackable' object has no attribute 'output_shapes'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-31bf89ff98cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmasking_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoTrackable' object has no attribute 'output_shapes'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}